{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f2d3fd-781a-472b-b29a-c99bc3df5b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:OLLAMA_API_ENDPOINT = http://ollama.ollama\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "import weaviate.classes as wvc\n",
    "import weaviate\n",
    "from weaviate.auth import AuthApiKey\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import weaviate\n",
    "\n",
    "ollama_api_endpoint = os.getenv(\"OLLAMA_HOST\", \"http://ollama.ollama\")\n",
    "ollama_vectorizer_model = model = \"all-minilm\"\n",
    "ollama_generative_model=\"llama3.2\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(f'OLLAMA_API_ENDPOINT = {ollama_api_endpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2814fc4-5762-4665-9343-a5f4ff637e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def connect_weaviate_embedded():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logging.info('Connecting to Weaviate embedded instance')\n",
    "    client = weaviate.connect_to_embedded(\n",
    "        environment_variables={\"ENABLE_MODULES\": \"text2vec-ollama,generative-ollama\"},\n",
    "        version=\"1.25.6\"\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d43ac6-7a50-469e-ba87-e5a4398a8994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connecting to Weaviate embedded instance\n",
      "INFO:weaviate-client:Started /opt/app-root/src/.cache/weaviate-embedded: process ID 203\n",
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"open cluster service\",\"servers\":{\"Embedded_at_8079\":53779},\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"address\":\"10.128.2.33:53780\",\"level\":\"info\",\"msg\":\"starting cloud rpc server ...\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"starting raft sub-system ...\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"address\":\"10.128.2.33:53779\",\"level\":\"info\",\"msg\":\"tcp transport\",\"tcpMaxPool\":3,\"tcpTimeout\":10000000000,\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"loading local db\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"database has been successfully loaded\",\"n\":0,\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"level\":\"info\",\"metadata_only_voters\":false,\"msg\":\"construct a new raft node\",\"name\":\"Embedded_at_8079\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"action\":\"raft\",\"index\":17,\"level\":\"info\",\"msg\":\"raft initial configuration\",\"servers\":\"[[{Suffrage:Voter ID:Embedded_at_8079 Address:10.129.0.159:52729}]]\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"last_snapshot_index\":0,\"last_store_applied_index\":0,\"last_store_log_applied_index\":47,\"level\":\"info\",\"msg\":\"raft node constructed\",\"raft_applied_index\":0,\"raft_last_index\":47,\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"action\":\"raft\",\"follower\":{},\"leader-address\":\"\",\"leader-id\":\"\",\"level\":\"info\",\"msg\":\"raft entering follower state\",\"time\":\"2024-11-12T20:43:07Z\"}\n",
      "{\"action\":\"raft\",\"last-leader-addr\":\"\",\"last-leader-id\":\"\",\"level\":\"warning\",\"msg\":\"raft heartbeat timeout reached, starting election\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"raft\",\"level\":\"info\",\"msg\":\"raft entering candidate state\",\"node\":{},\"term\":14,\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"raft\",\"level\":\"info\",\"msg\":\"raft election won\",\"tally\":1,\"term\":14,\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"raft\",\"leader\":{},\"level\":\"info\",\"msg\":\"raft entering leader state\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"level\":\"info\",\"msg\":\"reload local db: update schema ...\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"index\":\"Question\",\"level\":\"info\",\"msg\":\"restore local index\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"raft\",\"command\":0,\"level\":\"info\",\"msg\":\"raft updating configuration\",\"server-addr\":\"10.128.2.33:53779\",\"server-id\":\"Embedded_at_8079\",\"servers\":\"[[{Suffrage:Voter ID:Embedded_at_8079 Address:10.128.2.33:53779}]]\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"hnsw_prefill_cache_async\",\"level\":\"info\",\"msg\":\"not waiting for vector cache prefill, running in background\",\"time\":\"2024-11-12T20:43:08Z\",\"wait_for_cache_prefill\":false}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard question_I9KVqoqTMdct in 1.96855ms\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-11-12T20:43:08Z\",\"took\":79451}\n",
      "{\"action\":\"bootstrap\",\"leader\":\"10.128.2.33:53779\",\"level\":\"info\",\"msg\":\"successfully joined cluster\",\"time\":\"2024-11-12T20:43:08Z\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50050\",\"time\":\"2024-11-12T20:43:09Z\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-11-12T20:43:09Z\"}\n",
      "{\"address\":\"10.128.2.33:53779\",\"level\":\"info\",\"msg\":\"current Leader\",\"time\":\"2024-11-12T20:43:09Z\"}\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/.well-known/openid-configuration \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/.well-known/ready \"HTTP/1.1 200 OK\"\n",
      "INFO:root:\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Found 1 Weaviate nodes.\n",
      "INFO:root:\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/nodes \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Node(git_hash='\\'\"$GITHASH\"\\'', name='Embedded_at_8079', shards=None, stats=None, status='HEALTHY', version='1.25.6')\n",
      "INFO:root:\n",
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/meta \"HTTP/1.1 200 OK\"\n",
      "INFO:root:client.get_meta(): {'hostname': 'http://127.0.0.1:8079', 'modules': {'generative-ollama': {'documentationHref': 'https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion', 'name': 'Generative Search - Ollama'}, 'text2vec-ollama': {'documentationHref': 'https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings', 'name': 'Ollama Module'}}, 'version': '1.25.6'}\n"
     ]
    }
   ],
   "source": [
    "client = connect_weaviate_embedded()\n",
    "\n",
    "if client.is_ready():\n",
    "    logging.info('')\n",
    "    logging.info(f'Found {len(client.cluster.nodes())} Weaviate nodes.')\n",
    "    logging.info('')\n",
    "    for node in client.cluster.nodes():\n",
    "        logging.info(node)\n",
    "        logging.info('')\n",
    "    logging.info(f'client.get_meta(): {client.get_meta()}')\n",
    "else:\n",
    "    logging.error(\"Client is not ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ca0be5-5f36-44a3-ba9c-085193782057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:8079/v1/schema \"HTTP/1.1 200 OK\"\n",
      "{\"action\":\"load_all_shards\",\"level\":\"error\",\"msg\":\"failed to load all shards: context canceled\",\"time\":\"2024-11-12T20:43:09Z\"}\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:8079/v1/schema/Question \"HTTP/1.1 200 OK\"\n",
      "{\"action\":\"telemetry_push\",\"level\":\"info\",\"msg\":\"telemetry started\",\"payload\":\"\\u0026{MachineID:4c382da9-b68c-4722-80a2-6506cf9b8b32 Type:INIT Version:1.25.6 NumObjects:0 OS:linux Arch:amd64 UsedModules:[generative-ollama text2vec-ollama]}\",\"time\":\"2024-11-12T20:43:09Z\"}\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8928ba-2ac9-4816-a484-cd942c1752bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"warning\",\"msg\":\"prop len tracker file /opt/app-root/src/.local/share/weaviate/question/EQmVZC1mEZiy/proplengths does not exist, creating new tracker\",\"time\":\"2024-11-12T20:43:14Z\"}\n",
      "{\"action\":\"hnsw_prefill_cache_async\",\"level\":\"info\",\"msg\":\"not waiting for vector cache prefill, running in background\",\"time\":\"2024-11-12T20:43:14Z\",\"wait_for_cache_prefill\":false}\n",
      "{\"level\":\"info\",\"msg\":\"Created shard question_EQmVZC1mEZiy in 1.126545ms\",\"time\":\"2024-11-12T20:43:14Z\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-11-12T20:43:14Z\",\"took\":55671}\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8079/v1/schema \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Request status_code = 403\n",
      "ERROR:root:Error getting Jeopardy questions!\n",
      "ERROR:root:status_code = 403, Reason: Forbidden\n"
     ]
    }
   ],
   "source": [
    "# lets create the collection, specifing our base url accordingling\n",
    "questions = client.collections.create(\n",
    "    \"Question\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_ollama(\n",
    "        api_endpoint=ollama_api_endpoint,\n",
    "        model=ollama_vectorizer_model\n",
    "    ),\n",
    "    generative_config=wvc.config.Configure.Generative.ollama(\n",
    "        api_endpoint=ollama_api_endpoint,\n",
    "        model=ollama_generative_model\n",
    "    )\n",
    ")\n",
    "\n",
    "resp = requests.get('https://raw.githubusercontent.com/databyjp/wv_demo_uploader/main/weaviate_datasets/data/jeopardy_1k.json')\n",
    "logging.info(f'Request status_code = {resp.status_code}')\n",
    "\n",
    "if resp.status_code == 200:\n",
    "    data = json.loads(resp.text)\n",
    "\n",
    "    question_objs = list()\n",
    "    for i, d in enumerate(data):\n",
    "        question_objs.append({\n",
    "            \"answer\": d[\"Answer\"],\n",
    "            \"question\": d[\"Question\"],\n",
    "            \"category\": d[\"Category\"],\n",
    "            \"air_date\": d[\"Air Date\"],\n",
    "            \"round\": d[\"Round\"],\n",
    "            \"value\": d[\"Value\"]\n",
    "    })\n",
    "\n",
    "    logging.info('Importing 1000 Questions...')\n",
    "    questions = client.collections.get(\"Question\")\n",
    "    questions.data.insert_many(question_objs)\n",
    "    logging.info('Finished Importing Questions')\n",
    "\n",
    "    logging.info(f'Collection: {questions}')\n",
    "else:\n",
    "    logging.error('Error getting Jeopardy questions!')\n",
    "    logging.error(f'status_code = {resp.status_code}, Reason: {resp.reason}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1650a-ae0b-4058-adc1-aed752b37a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def respond(query='computers', task='Summarize', limit=1) -> str:\n",
    "    print(f'\\nPerforming generative search, query = {query}, limit = {limit}.')\n",
    "    print(f'Prompt: {task}')\n",
    "    print(f'limit = {limit}')\n",
    "    response = questions.generate.near_text(\n",
    "        query=query,\n",
    "        limit=limit,\n",
    "        grouped_task=task\n",
    "    )\n",
    "    return response.generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd7e5d-3e17-4898-8484-dd911dd24735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Search the Jeopardy Vector Database. (powered by Weaviate and Ollama)\") as demo:\n",
    "            gr.Markdown(\"\"\"# Search and summarize the Jeopardy Vector Database. (Powered by Weaviate and Ollama)\"\"\")\n",
    "            semantic_examples = [\n",
    "                [\"Nature\"],\n",
    "                [\"Music\"],\n",
    "                [\"Wine\"],\n",
    "                [\"Consumer Products\"],\n",
    "                [\"Sports\"],\n",
    "                [\"Fishing\"],\n",
    "                [\"Food\"],\n",
    "                [\"Weather\"]\n",
    "            ]\n",
    "            semantic_input_text = gr.Textbox(label=\"Enter a search concept or choose an example below:\", \n",
    "                value=semantic_examples[0][0])\n",
    "            gr.Examples(semantic_examples, inputs=semantic_input_text, label=\"Example search concepts:\")\n",
    "            vdb_button = gr.Button(value=\"Search and Summarize the Jeopardy Vector Database.\")\n",
    "            vdb_button.click(fn=respond, inputs=[semantic_input_text], outputs=gr.Textbox(label=\"Search Results\"))\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name='0.0.0.0', server_port=8082, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc03b89-88e8-423e-8a7a-960656dec6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
